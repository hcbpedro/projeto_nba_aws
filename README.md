# Projeto: Pipeline de Dados e An√°lise de Desempenho da NBA na AWS

![Status: Projeto Conclu√≠do](https://img.shields.io/badge/status-conclu%C3%ADdo-brightgreen)

Este projeto de portf√≥lio demonstra a constru√ß√£o de um pipeline de dados de ponta a ponta na nuvem AWS para ingerir, processar, analisar e visualizar estat√≠sticas de desempenho de jogadores da NBA.

## üìä Dashboard Interativo no QuickSight

O produto final √© um dashboard interativo no Amazon QuickSight que permite a an√°lise de desempenho e efici√™ncia de jogadores por temporada. O painel inclui an√°lises de l√≠deres de estat√≠sticas, um estudo sobre efici√™ncia de arremessos (Volume vs. Efici√™ncia com TS%) e uma vis√£o da evolu√ß√£o da carreira de jogadores individuais.

_Dashboard Principal_
<img src="screenshots/dashboard_principal.png" alt="Dashboard Principal" width="800"/>

_An√°lise de Efici√™ncia (PPG vs. TS%)_
<img src="screenshots/dashboard_eficiencia.png" alt="An√°lise de Efici√™ncia" width="800"/>

_An√°lise de Carreira_
<img src="screenshots/dashboard_carreira.png" alt="An√°lise de Carreira" width="800"/>


## ‚öôÔ∏è Arquitetura da Solu√ß√£o

O pipeline foi constru√≠do utilizando uma arquitetura serverless e escal√°vel na AWS:

**S3 (Data Lake) ‚Üí AWS Glue (ETL com PySpark) ‚Üí S3 (Dados Processados em Parquet) ‚Üí AWS Glue Crawler ‚Üí Amazon Athena ‚Üí Amazon QuickSight (SPICE)**

## üõ†Ô∏è Tecnologias Utilizadas

* **Cloud:** AWS (S3, Glue, Athena, QuickSight, IAM)
* **Linguagem & Biblioteca:** Python, PySpark
* **Consulta de Dados:** SQL (via Amazon Athena)
* **Formatos de Dados:** CSV (bruto), Parquet (processado)

## üöÄ Processo do Projeto

1.  **Engenharia de Dados (ETL):** Um job no AWS Glue, escrito em PySpark, √© respons√°vel por:
    * Ler os dados brutos (CSV) de um Data Lake no S3.
    * Realizar a limpeza e o tratamento de dados, incluindo a corre√ß√£o de um cabe√ßalho malformado.
    * Agregar estat√≠sticas por jogador e por temporada.
    * Calcular m√©tricas de desempenho (PPG, RPG, APG) e de efici√™ncia (FG%, FG3%, FT%, TS%).
    * Salvar o dataset enriquecido e limpo de volta no S3 em formato Parquet, otimizado para an√°lises.

2.  **Disponibiliza√ß√£o para An√°lise:**
    * Um Crawler do AWS Glue cataloga os dados processados, criando uma tabela no AWS Data Catalog.
    * O Amazon Athena utiliza essa tabela para permitir consultas SQL diretamente nos arquivos Parquet.

3.  **Visualiza√ß√£o de Dados:**
    * O Amazon QuickSight se conecta ao Athena e importa os dados para o seu motor em mem√≥ria (SPICE) para garantir alta performance.
    * O dashboard interativo foi constru√≠do com filtros, tabelas e gr√°ficos detalhados para permitir uma explora√ß√£o aprofundada dos dados.

## üí° Desafios T√©cnicos e Solu√ß√µes

* **Parsing de CSV Malformado:** O principal arquivo de dados possu√≠a um cabe√ßalho com uma v√≠rgula inicial, o que impedia a infer√™ncia de esquema do Spark. A solu√ß√£o foi ler o arquivo sem cabe√ßalho, remover a primeira linha programaticamente via RDD e nomear as colunas manualmente no script, tornando o carregamento robusto.
* **Depura√ß√£o de Permiss√µes IAM no S3:** Durante o desenvolvimento, o job falhava na segunda execu√ß√£o ao tentar sobrescrever (`overwrite`) os dados. A investiga√ß√£o revelou que a opera√ß√£o de `overwrite` do Spark requer n√£o apenas `s3:PutObject` e `s3:DeleteObject`, mas tamb√©m a permiss√£o `s3:ListBucket` no prefixo de destino. A pol√≠tica IAM foi ajustada para refletir isso. Para fins de aprendizado, a pol√≠tica `AmazonS3FullAccess` foi utilizada para focar na engenharia de dados, com a ci√™ncia de que uma pol√≠tica restritiva √© a melhor pr√°tica em produ√ß√£o.

---
